{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ae0270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "104cbb73",
   "metadata": {},
   "source": [
    "# Mood & Time Aware Content Recommendation System\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1647e",
   "metadata": {},
   "source": [
    "## 1. Problem Definition & Objective\n",
    "\n",
    "### a. Selected Project Track\n",
    "**AI/ML - Personalized Content Recommendation System**\n",
    "\n",
    "### b. Clear Problem Statement\n",
    "Users often experience decision fatigue or find themselves consuming content that doesn't align with their current emotional state or time of day. Most platforms recommend content based solely on past history, ignoring the user's *immediate* mood. Anshika aims to bridge this gap by suggesting personalized activities and YouTube videos based on a real-time assessment of user mood (Focus, Relax, Bored, or Sad) and the current time of day.\n",
    "\n",
    "### c. Real-world Relevance and Motivation\n",
    "In an era of information overload, mental well-being is closely tied to digital consumption. Suggesting a high-intensity action movie to someone feeling 'Sad' might be counterproductive, whereas an uplifting music video could improve their mood. Anshika motivates balanced living by adapting the digital environment to the user's psychological state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb50ef",
   "metadata": {},
   "source": [
    "## 2. Data Understanding & Preparation\n",
    "\n",
    "### a. Dataset Source\n",
    "We use **Synthetic Data** generated through LLM simulation to represent user usage patterns (Time of Day, Past Moods, Content Types). For content recommendations, we use the **YouTube Data API** (simulated via AI flows) to fetch relevant video metadata.\n",
    "\n",
    "### b. Data Loading and Exploration\n",
    "Below is a simulation of the data structure used for mood prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15952d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample User Activity Data:\n",
      "  time_of_day past_moods         content_types\n",
      "0     Morning      Focus       Coding Tutorial\n",
      "1   Afternoon      Bored                Comedy\n",
      "2     Evening      Relax           Lo-fi Music\n",
      "3       Night        Sad         Uplifting Pop\n",
      "4     Morning      Focus  Productivity Podcast\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Mock dataset representing user history\n",
    "data = {\n",
    "    \"time_of_day\": [\"Morning\", \"Afternoon\", \"Evening\", \"Night\", \"Morning\"],\n",
    "    \"past_moods\": [\"Focus\", \"Bored\", \"Relax\", \"Sad\", \"Focus\"],\n",
    "    \"content_types\": [\"Coding Tutorial\", \"Comedy\", \"Lo-fi Music\", \"Uplifting Pop\", \"Productivity Podcast\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Sample User Activity Data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5676d",
   "metadata": {},
   "source": [
    "### c. Cleaning, Preprocessing, Feature Engineering\n",
    "The input data is structured into a JSON format expected by the Genkit `suggestMoodFlow`. Feature engineering involves categorical encoding of 'Time of Day' and 'Moods' which the LLM processes as semantic tokens.\n",
    "\n",
    "### d. Handling Missing Values\n",
    "Missing history is handled by defaulting to a 'Neutral/Bored' state or providing a diverse mix of content until a pattern is established."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe4b361",
   "metadata": {},
   "source": [
    "## 3. Model / System Design\n",
    "\n",
    "### a. AI Technique Used\n",
    "**Hybrid Approach**: Large Language Models (LLM) using Google Gemini for semantic reasoning (Mood classification and Content matching) integrated with a structured pipeline via **Firebase Genkit** for orchestration.\n",
    "\n",
    "### b. Architecture or Pipeline Explanation\n",
    "1. **Input Layer**: Captures current time and recent user interactions.\n",
    "2. **Mood Flow (`auto-mood-suggestion`)**: LLM analyzes history to predict current mood state.\n",
    "3. **Recommendation Flow (`youtube-video-suggester`)**: LLM uses predicted mood to search/generate 9 relevant video recommendations.\n",
    "4. **Output Layer**: Dynamic Dashboard updates UI with personalized suggestions.\n",
    "\n",
    "### c. Justification of Design Choices\n",
    "- **Genkit**: Provides strongly typed AI flows, making it robust for production use.\n",
    "- **LLM for Prediction**: Unlike traditional ML classifiers, LLMs can interpret complex semantic relationships between varied content types (e.g., 'Lo-fi' suggesting a 'Relax' mood)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7193d5",
   "metadata": {},
   "source": [
    "## 4. Core Implementation\n",
    "\n",
    "### a. Model Training / Inference Logic\n",
    "Since we use a pre-trained LLM (Gemini 1.5), \"training\" is replaced by fine-tuning prompts. Below is the simulation of the Inference Logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09634ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Result: {'suggestedMood': 'Relax', 'confidence': 0.92}\n"
     ]
    }
   ],
   "source": [
    "def simulate_mood_inference(time_of_day, past_moods):\n",
    "    \"\"\"Simulates the logic in src/ai/flows/auto-mood-suggestion.ts\"\"\"\n",
    "    if time_of_day == \"Night\" and \"Relax\" in past_moods:\n",
    "        return {\"suggestedMood\": \"Relax\", \"confidence\": 0.92}\n",
    "    elif \"Bored\" in past_moods:\n",
    "        return {\"suggestedMood\": \"Focus\", \"confidence\": 0.75} # Pivot to focus for productivity\n",
    "    return {\"suggestedMood\": \"Relax\", \"confidence\": 0.50}\n",
    "\n",
    "current_prediction = simulate_mood_inference(\"Night\", [\"Focus\", \"Relax\"])\n",
    "print(f\"Inference Result: {current_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff49cf9",
   "metadata": {},
   "source": [
    "### b. Prompt Engineering\n",
    "The core of Anshika lies in its prompt structure:\n",
    "```\n",
    "System: You are an empathetic content curator.\n",
    "Input: {timeOfDay, pastMoods, contentHistory}\n",
    "Output: Suggest one of [Focus, Relax, Bored, Sad].\n",
    "```\n",
    "For video suggestions, we use **Few-Shot Prompting** to ensure the JSON output contains valid YouTube metadata format.\n",
    "\n",
    "### c. Recommendation Pipeline\n",
    "The pipeline executes `suggestMoodFlow` -> `suggestVideosFlow` sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719ab175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for Relax:\n",
      "[\n",
      "  {\n",
      "    \"title\": \"Uplifting Relax Music\",\n",
      "    \"id\": \"dQw4w9WgXcQ\",\n",
      "    \"desc\": \"Best curated list.\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"How to stay Relax\",\n",
      "    \"id\": \"ABC123XYZ\",\n",
      "    \"desc\": \"Expert tips.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "def get_recommendations(mood):\n",
    "    # Simulated response from src/ai/flows/youtube-video-suggester.ts\n",
    "    return [\n",
    "        {\"title\": f\"Uplifting {mood} Music\", \"id\": \"dQw4w9WgXcQ\", \"desc\": \"Best curated list.\"},\n",
    "        {\"title\": f\"How to stay {mood}\", \"id\": \"ABC123XYZ\", \"desc\": \"Expert tips.\"}\n",
    "    ]\n",
    "\n",
    "print(f\"Recommendations for {current_prediction['suggestedMood']}:\")\n",
    "print(json.dumps(get_recommendations(current_prediction['suggestedMood']), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001a8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f825d74",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Analysis\n",
    "\n",
    "### a. Metrics Used\n",
    "- **Quantitative**: Confidence score (0-1) returned by the model.\n",
    "- **Qualitative**: Relevance checkâ€”does 'Sad' mood produce 'Uplifting' content?\n",
    "\n",
    "### b. Sample Outputs\n",
    "Input: `{ \"mood\": \"Sad\" }` -> AI returns \"Uplifting Pop\" and \"Cute Animal Compilations\".\n",
    "\n",
    "### c. Performance Analysis and Limitations\n",
    "- **Performance**: LLM inference takes ~1-2s, manageable for real-time app use.\n",
    "- **Limitations**: Highly dependent on the quality of user input history; occasional hallucination of YouTube video IDs (mitigated by Genkit schema validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb3842",
   "metadata": {},
   "source": [
    "## 6. Ethical Considerations & Responsible AI\n",
    "\n",
    "### a. Bias and Fairness Considerations\n",
    "The system avoids suggesting harmful or sensitive content in response to negative moods (like 'Sad'). It prioritizes clinically safe, general-purpose uplifting content.\n",
    "\n",
    "### b. Dataset Limitations\n",
    "Synthetic data may not capture the full complexity of human emotions. The model is restricted to 4 major categories to maintain accuracy.\n",
    "\n",
    "### c. Responsible use of AI tools\n",
    "Anshika includes a disclaimer that it is a *companion* app and not a substitute for professional mental health advice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb30a8",
   "metadata": {},
   "source": [
    "## 7. Conclusion & Future Scope\n",
    "\n",
    "### a. Summary of Results\n",
    "Anshika successfully implements a mood-aware content pipeline using Genkit and Gemini. It demonstrates that combining time-of-day data with semantic user history allows for highly personalized digital experiences.\n",
    "\n",
    "### b. Possible Improvements and Extensions\n",
    "- **Real API Integration**: Connect to YouTube Data API for real-time video validation.\n",
    "- **Multi-Modal Input**: Use facial expression analysis (via camera) to determine mood more accurately.\n",
    "- **Extended Ecosystem**: Suggest podcasts, Spotify playlists, and Pomodoro timers integrated into the flow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
